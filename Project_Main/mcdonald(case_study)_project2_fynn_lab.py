# -*- coding: utf-8 -*-
"""McDonald(case study)_project2_Fynn Lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKW71jwIPixglvHPcSBX0-CGUI3OaScl
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import matplotlib.pyplot as plt
# %matplotlib inline
!pip install bioinfokit
import bioinfokit as bio
import seaborn as sns
from sklearn.preprocessing import StandardScaler
scalar=StandardScaler()
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN,SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn import tree
from sklearn import metrics
import plotly.express as px
import plotly
plotly.offline.init_notebook_mode(connected=True)

macd=pd.read_csv("/content/mcdonalds.csv")
macd.head()

macd.shape

macd.columns.values

macd.info()

macd.describe(include="all")

macd.isna().sum()
macd.isnull().sum()

print(macd.iloc[:, [0, 1, 3, 5, 7, 8, 13]].describe())

macd['VisitFrequency'].value_counts()

macd["yummy"]= macd["yummy"].replace({'Yes':1, 'No':0})
macd["convenient"] = macd["convenient"].replace({'Yes':1, 'No':0})
macd["spicy"] = macd["spicy"].replace({'Yes':1, 'No':0})
macd["fattening"] = macd["fattening"].replace({'Yes':1, 'No':0})
macd["greasy"] = macd["greasy"].replace({'Yes':1, 'No':0})
macd["fast"] = macd["fast"].replace({'Yes':1, 'No':0})
macd["cheap"] = macd["cheap"].replace({'Yes':1, 'No':0})
macd["tasty"] = macd["tasty"].replace({'Yes':1, 'No':0})
macd["expensive"] = macd["expensive"].replace({'Yes':1, 'No':0})
macd["healthy"] = macd["healthy"].replace({'Yes':1, 'No':0})
macd["disgusting"] = macd["disgusting"].replace({'Yes':1, 'No':0})
macd['Like']= macd['Like'].replace({'I hate it!-5': '-5','I love it!+5':'+5'})

macd.head()

mac1=macd.iloc[:,0:11]
mac1.head()

plt.rcParams['figure.figsize'] = (25, 8)
f = sns.countplot(x=macd['Age'],palette = 'Blues')
f.bar_label(f.containers[0])
plt.title('Age of customers')
plt.show()

from sklearn.preprocessing import LabelEncoder
def label(x):
    macd[x] = LabelEncoder().fit_transform(macd[x])
    return macd

category = ['yummy', 'convenient', 'spicy', 'fattening', 'greasy', 'fast', 'cheap',
       'tasty', 'expensive', 'healthy', 'disgusting']

for i in category:
    label(i)
macd

mcd = macd.loc[:,category]
mcd

x = macd.loc[:,category].values
print(x)

from sklearn import preprocessing
pca_data = preprocessing.scale(x)

pca = PCA(n_components=11)
pc = pca.fit_transform(x)
names = ['A','B','C','D','E','F','G','H','I','J','K']
pf = pd.DataFrame(data = pc, columns = names)
pf

pca.explained_variance_ratio_

np.cumsum(pca.explained_variance_ratio_)

loadings = pca.components_
num_pc = pca.n_features_
pc_list = [chr(i) for i in range(ord('A'),ord('K')+1)]
loadings_mac = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))
loadings_mac['variable'] = mcd.columns.values
loadings_mac = loadings_mac.set_index('variable')
loadings_mac

plt.rcParams['figure.figsize'] = (20,15)
ax = sns.heatmap(loadings_mac, annot=True, cmap='coolwarm')
plt.show()

from bioinfokit.visuz import cluster
cluster.screeplot(obj=[pc_list, pca.explained_variance_ratio_],show=True,dim=(10,5))

std_dev = []
for i in names:
    std_dev.append(np.std(pf[i]))

print("Standard Deviation")
np.array(std_dev)

cp = np.cumsum(pca.explained_variance_ratio_)
print("Cumulative Proportion")
cp

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,11)).fit(mcd)
visualizer.show()

kmeans = KMeans(n_clusters=3, init='k-means++', random_state=0).fit(mcd)
macd['cluster_num'] = kmeans.labels_
print ("kmeans.labels_",kmeans.labels_) 
print ("kmeans.inertia_",kmeans.inertia_)  
print("kmeans.n_iter_",kmeans.n_iter_)

print("kmeans.cluster_centers_",kmeans.cluster_centers_)

from collections import Counter
Counter(kmeans.labels_)

sns.scatterplot(data=pf, x="A", y="B", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 
            marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()

mcd_data2=pd.get_dummies(macd, prefix=['cluster_num'], columns=['cluster_num'])
mcd_data2.head(4)

from statsmodels.graphics.mosaicplot import mosaic
from itertools import product

crosstab =pd.crosstab(macd['cluster_num'],macd['Like'])
crosstab = crosstab[['-5','-4','-3','-2','-1','0','+1','+2','+3','+4','+5']]
crosstab

plt.rcParams['figure.figsize'] = (7,5)
mosaic(crosstab.stack())
plt.show()

crosstab_gender =pd.crosstab(macd['cluster_num'],macd['Gender'].replace({1: 'Male', 0: 'Female'}))
crosstab_gender

plt.rcParams['figure.figsize'] = (7,5)
fig, ax = plt.subplots(figsize=(10, 8))
mosaic(crosstab_gender.stack(), ax=ax, title='Gender vs Cluster Number')
plt.show()

macd['VisitFrequency'] = LabelEncoder().fit_transform(macd['VisitFrequency'])
visit = macd.groupby('cluster_num')['VisitFrequency'].mean()
visit = visit.to_frame().reset_index()
visit

macd['Like'] = LabelEncoder().fit_transform(macd['Like'])
Like = macd.groupby('cluster_num')['Like'].mean()
Like = Like.to_frame().reset_index()
Like

macd['Gender'] = LabelEncoder().fit_transform(macd['Gender'])
Gender = macd.groupby('cluster_num')['Gender'].mean()
Gender = Gender.to_frame().reset_index()
Gender

segment = Gender.merge(Like, on='cluster_num', how='left').merge(visit, on='cluster_num', how='left')
segment

scaled_mac = scalar.fit_transform(mac1)

pca = PCA(n_components=11)
principal_components = pca.fit_transform(scaled_mac)
pca_mac = pd.DataFrame(data=principal_components ,columns=["A","B","C","D","E","F","G","H","I","J","K"])
pca_mac.head()

sns.scatterplot(data=pca_mac, x="A", y="B")

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,10))
visualizer.fit(scaled_mac)       
visualizer.show()

from yellowbrick.cluster import SilhouetteVisualizer
model = KMeans(3)
visualizer = SilhouetteVisualizer(model)
visualizer.fit(scaled_mac)
visualizer.show()

kmeans_model=KMeans(3)
kmeans_model.fit_predict(scaled_mac)
pca_mac_kmeans= pd.concat([pca_mac,pd.DataFrame({'cluster':kmeans_model.labels_})],axis=1)
pca_mac_kmeans.head()

plt.figure(figsize=(8,8))
ax=sns.scatterplot(x="A",y="B",hue="cluster",data=pca_mac_kmeans,palette=['red','green','blue'])
plt.title("K-Means Clustering")
plt.show()

kmeans_model=KMeans(3)
kmeans_model.fit_predict(scaled_mac)
pca_mac_kmeans1= pd.concat([mac1,pd.DataFrame({'cluster':kmeans_model.labels_})],axis=1)
pca_mac_kmeans1.head()

sns.countplot(x ='cluster', data = pca_mac_kmeans1,hue = "cluster")

cluster_1_mac = pca_mac_kmeans1[pca_mac_kmeans1["cluster"]==0]
cluster_1_mac.head()

cluster_2_mac = pca_mac_kmeans1[pca_mac_kmeans1["cluster"]==1]
cluster_2_mac.head()

cluster_3_mac = pca_mac_kmeans1[pca_mac_kmeans1["cluster"]==2]
cluster_3_mac.head()

mac_new= pd.concat([macd,pd.DataFrame({'cluster':kmeans_model.labels_})],axis=1)
mac_new.head()

mac_new['VisitFrequency'].value_counts()

mac_new["VisitFrequency"]= mac_new["VisitFrequency"].replace({'Never':0, 'Once a year':1, 'Every three months':2, 'Once a month':3, 'Once a week':4, 'More than once a week':5})
mac_new['Like'].value_counts()

mac_new['Gender'].value_counts()

mac_new["Like"]= mac_new["Like"].replace({'I hate it!-5':0, '-4':1, '-3':2, '-2':3, '-1':4, '0':5, '+1':6, '+2':7, '+3':8, '+4':9, 'I love it!+5':10})
mac_new["Gender"]= mac_new["Gender"].replace({'Female':0, 'Male':1})
mac_new

X = mac_new.drop(['cluster'],axis=1)
y= mac_new[['cluster']]
X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3)

from sklearn.ensemble import RandomForestClassifier
# Create a Random Forest classifier
# Train the model on the training dataset by using fit() function
RForest_clf = RandomForestClassifier(n_estimators = 100)
RForest_clf.fit(X_train, y_train)

# Predict from the test dataset
y_pred = RForest_clf.predict(X_test)

# Import metrics for accuracy calculation
from sklearn import metrics
print('\n'"Accuracy of our Random Forest Classifier for this dataset is : ",
metrics.accuracy_score(y_test, y_pred)*100)